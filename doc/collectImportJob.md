## **Collect Import Job**

Collect Survey definition (.collect) and full backups including survey definition and data (.collect-backup) files can be imported.

Steps:

1. import survey definition file
   - read survey definition (idml.xml file) into a json object file to easily access it
   - put survey definition json object into job context to make it available to other jobs
   - create and store Arena survey using Collect survey name, label, descriptions
1. import Collect code lists into Arena categories:
   - for every code list create and store a category (skip sampling point data code list: will be imported later on with another inner job)
   - import category levels from the Collect hierarchy/level elements
   - insert items recursively and in batch
   - if an item is "qualifiable", put its code in an object to be passed to the job context (to be used by other jobs)
1. import taxonomies
   - taxonomies are stored in the Collect survey file in a subfolder called "species"
   - every taxonomy is stored in a separate .csv file: the name of the file is the name of the species list
   - import every file into a new taxonomy in Arena
     1. read the file header to extract the vernacular language codes
     1. import every row into a new Taxon
        - validate the row before importing it:
        - code, family, scientific_name are mandatory
        - skip rows without code (automatically generated by Collect)
        - store code and scientific_name into an object to easily check duplicate items
        - read columns code, family, scientific_name
        - extract the vernacular names from the vernacular language code columns
        - create a new Taxon object
        - add taxon to the insert buffer
     1. finalize import (flush insert buffer)
   - add the imported taxonomies to the job context
1. import sampling point data into a new category:
   - in Collect survey file, sampling point data is stored in the file "sampling_design/sampling_design.csv"
   - import the sampling point data as a normal category
   - generate import summary from file (to generate the item extra props definition)
   - import every row into an Arena CategoryItem
   - merge srs_id, x and y extra columns into a Point object and put it into a "location" extra prop
1. import node definitions:
   - node definitions are stored in the Collect survey definition file, under the "schema" element
   - import node definitions following their hierarchy, starting from the root definition
   - for every node definition create a NodeDef of the corresponding type:
     - copy properties from Collect node definition
     - make node def name unique (Collect node def names are only unique relatively to the same parent entity)
     - import node def type specific props (categoryUuid for code, taxonomyUuid for taxon, etc.)
     - import advanced props
       - default values
       - validation rules
       - applicable (relevant when)
     - if the node def is of type code and the associated code list is "qualifiable", create a new node def of type text to store the "specify" value (make it relevant only when the qualifiable code list / category item is selected)
1. generate survey dependency graph
1. import records
   - Collect records are stored in a folder "data" and are divided into subfolders for each workflow step (1=ENTRY, 2=CLEANSING, 3=ANALYSIS)
   - every record is stored in a XML file
   - the file is in a human readable format: every element name corresponds to the name of a node definition
   - import only records in their last step
   1. determine record entry names (file names of first non empty step folder)
   1. for every entry name, find record data (starting from step 3 to step 1)
   1. create Arena record
   1. import record nodes traversing Collect record with a BFS algorithm:
      - get the node definition from the element name
      - extract node value and metadata
      - create Arena node and persist it (in batch)
      - if node is an attribute, validate it and store the validation in the Arena record validation object
      - if node is multiple, validate min and max children count
   1. persist record validation
1. generate RDB
